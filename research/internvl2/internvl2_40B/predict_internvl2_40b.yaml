seed: 0
output_dir: './output' # path to save checkpoint/strategy
load_checkpoint: "/path/to/internvl2_40b/"
auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
transform_process_num: 2
only_save_strategy: False
resume_training: False
use_parallel: True
run_mode: &run_mode 'predict'
# trainer config
trainer:
  type: MultiModalToTextGenerationTrainer
  model_name: 'internvl2'

# default parallel of device num = 8 for Atlas 800T A2
parallel_config:
  data_parallel: 1
  model_parallel: 2
  pipeline_stage: 1
  use_seq_parallel: False
  micro_batch_num: 1
  vocab_emb_dp: True
  gradient_aggregation_group: 4
# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
micro_batch_interleave_num: 1

model:
  arch:
    type: InternVLChatModel
    auto_register: internvl.InternVLChatModel
  model_config:
    type: InternVLChatConfig
    auto_register: internvl_config.InternVLChatConfig
    freeze_llm: True
    batch_size: 1
    seq_length: &seq_length 4096
    max_length: *seq_length
    num_queries: &num_queries 256
    use_past: True
    is_dynamic: True
    bos_token_id: 1
    eos_token_id: 7
    pad_token_id: 0
    do_sample: True
    block_size: 16
    num_blocks: 512
    top_k: 50
    top_p: 0.8
    max_decode_length: 256
    compute_dtype: "bfloat16"
    layernorm_compute_type: "float32"
    softmax_compute_type: "float32"
    rotary_dtype: "bfloat16"
    param_init_type: "bfloat16"

    vision_model:
      arch:
        type: InternVisionModel
        auto_register: intern_clip_vit.InternVisionModel
      model_config:
        type: InternVisionConfig
        auto_register: internvl_config.InternVisionConfig
        model_type: intern_vit_6b
        norm_type: rms_norm
        hidden_size: 3200
        intermediate_size: 12800
        num_hidden_layers: 45
        num_attention_heads: 25
        use_flash_attention: False
        image_size: &image_size 448
        patch_size: 14
        num_channels: 3
        hidden_act: "gelu"
        dropout: 0.0
        attention_dropout: 0.0
        initializer_range: 1e-10
        initializer_factor: 0.1
        layer_norm_eps: 1.e-6
        qk_normalization: true
        qkv_bias: false
        checkpoint_name_or_path: ""

    text_model:
      arch:
        type: LlamaForCausalLM
      model_config:
        type: LlamaConfig
        seq_length: *seq_length
        hidden_size: 7168
        num_layers: 60
        num_heads: 56
        n_kv_heads: 8
        vocab_size: 64007
        max_new_tokens: 256
        intermediate_size: 20480
        rms_norm_eps: 1.0e-5
        emb_dropout_prob: 0.0
        use_flash_attention: True
        use_past_shard: False
        offset: 0
        scaling_factor: 1.0 # The scale factor of seq length
        ignore_token_id: -100
        repetition_penalty: 1
        max_length: *seq_length
        checkpoint_name_or_path: ""
        theta: 5000000.0
        block_size: 128
        num_blocks: 2048


# processor
processor:
  type: BaseXModalToTextProcessor
  max_length: *seq_length
  add_special_tokens: True
  model_transform_template:
    type: InternVLImageContentTransformTemplate
    auto_register: internvl2_processor.InternVLImageContentTransformTemplate
    image_size: *image_size
    context_length: *num_queries
    text_instruct_mode: True
    template: "Hermes-2"
    output_columns: [ "input_ids", "images", "image_context_pos", "no_image_tag"]
    vstack_columns: [ "images", "image_context_pos"]
    img_context_token: '<IMG_CONTEXT>'
    context_pad_token_id: 64000
  tokenizer:
    add_bos_token: False
    add_eos_token: False
    add_special_tokens: True
    unk_token: "<unk>"
    bos_token: "<|startoftext|>"
    eos_token: "<|im_end|>"
    pad_token: "<unk>"
    content: "<IMG_CONTEXT>"
    vocab_file: "/path/to/internvl2-40b/huggenface/tokenizer.model"
    type: InternvlTokenizer
    auto_register: internvl_tokenizer.InternvlTokenizer

# mindspore context init config
context:
  ascend_config:
    precision_mode:
      "must_keep_origin_dtype"
  max_call_depth: 10000
  max_device_memory: "59GB"
  device_id: 0
