seed: 0
output_dir: './output' # path to save checkpoint/strategy
run_mode: 'predict'
use_parallel: False

load_checkpoint: ""
auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model

# trainer config
trainer:
  type: MultiModalToTextGenerationTrainer
  model_name: 'mllama'

# default parallel of device num = 8 for Atlas 800T A2
parallel_config:
  model_parallel: 2
  pipeline_stage: 1

model:
  model_config:
    type: MllamaConfig
    stage: 2 #1--Pretrain Stage; 2--Finetune Stage
    freeze_vision: True
    batch_size: 1
    seq_length: &seq_length 2048
    compute_dtype: "bfloat16"
    layernorm_compute_type: "float32"
    softmax_compute_type: "float32"
    rotary_dtype: "bfloat16"
    param_init_type: "bfloat16"
    ignore_token_id: &ignore_token_id -100
    bos_token_id: 128000
    pad_token_id: 128004
    eos_token_id: [128001, 128008, 128009]
    repetition_penalty: 1
    use_past: True
    block_size: 16
    num_blocks: 512
    is_dynamic: &is_dynamic True
    top_k: 0
    top_p: 0.8
    max_decode_length: 1024
    do_sample: False
    use_flash_attention: True
    vision_model:
      arch:
        type: MllamaVisionModel
      model_config:
        type: MllamaVisionConfig
        hidden_size: 1280
        intermediate_size: 5120
        num_attention_heads: 16
        image_size: &image_size 560
        patch_size: 14
        hidden_act: "gelu"
        intermediate_layers_indices: [3, 7, 15, 23, 30]
        length_penalty: 1.0
        max_length: 20
        max_num_tiles: 4
        min_length: 0
        model_type: mllama_vision_model
        no_repeat_ngram_size: 0
        norm_eps: 1.0e-05
        num_beam_groups: 1
        num_beams: 1
        num_channels: 3
        max_num_images: &max_num_images 1
        num_global_layers: 8
        num_hidden_layers: 32
        num_return_sequences: 1
        output_attentions: false
        output_hidden_states: false
        output_scores: false
        patch_size: 14
        supported_aspect_ratios: [[1, 1], [1, 2], [1, 3], [1, 4], [2, 1], [2, 2], [3, 1], [4, 1]]
        temperature: 1.0
        tf_legacy_loss: false
        tie_encoder_decoder: false
        tie_word_embeddings: true
        top_k: 50
        top_p: 1.0
        torchscript: false
        typical_p: 1.0
        use_bfloat16: false
        vision_output_dim: 7680
    text_model:
      arch:
        type: MllamaForCausalLM
      model_config:
        type: MllamaTextConfig
        image_token_index: 128256
        add_cross_attention: false
        chunk_size_feed_forward: 0
        cross_attention_layers: [3, 8, 13, 18, 23, 28, 33, 38]
        hidden_size: 4096
        intermediate_size: 14336
        length_penalty: 1.0
        max_length: 20
        max_position_embedding: 131072
        min_length: 0
        model_type: mllama_text_model
        no_repeat_ngram_size: 0
        num_heads: 32
        num_layers: 40
        n_kv_heads: 8
        num_beam_groups: 1
        num_beams: 1
        num_return_sequences: 1
        output_attentions: false
        output_hidden_states: false
        output_scores: false
        pad_token_id: 128004
        rms_norm_eps: 1.0e-05
        extend_method: "LLAMA3"
        scaling_factor: 
          factor: 8.0
          high_freq_factor: 4.0
          low_freq_factor: 1.0
          original_max_position_embeddings: 8192
        theta: 500000.0
        vocab_size: 128256
        rms_norm_eps: 1.0e-5
        emb_dropout_prob: 0.0
        bos_token_id: 1
        eos_token_id: 2
        use_past_shard: False
        offset: 0
        checkpoint_name_or_path: ""
        do_sample: False
  arch:
    type: MllamaForConditionalGeneration

# processor
processor:
  type: BaseXModalToTextProcessor
  max_length: *seq_length
  model_transform_template:
    type: MllamaProcessor
    add_special_tokens: True
    context_length: *seq_length
    image_size: *image_size
    max_num_images: *max_num_images
    # text_instruct_mode: True
    output_columns: [ "input_ids", "pixel_values", "aspect_ratio_ids", "aspect_ratio_mask", "cross_attention_mask", "labels" ]
    mode: 'predict'
    ignore_token_id: *ignore_token_id
    image_mean: [0.48145466, 0.4578275, 0.40821073]
    image_std: [0.26862954,0.26130258, 0.27577711]
  tokenizer:
    add_bos_token: True
    add_eos_token: False
    vocab_file: ""
    type: MllamaTokenizer

# mindspore context init config
context:
  mode: 0 #0--Graph Mode; 1--Pynative Mode
  ascend_config:
    precision_mode:
      "must_keep_origin_dtype"
  max_call_depth: 10000
  max_device_memory: "58GB"
  save_graphs: False
  save_graphs_path: "./graph"
  device_id: 0

profile: False
profile_start_step: 1
profile_stop_step: 10
init_start_profile: False
profile_communication: False
profile_memory: True
