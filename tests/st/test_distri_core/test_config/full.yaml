training_config:
  epoch: 10
  batch_size: 5
  seed: 42

parallel_config:
  tensor_parallel: 1
  pipeline_stage: 1
  context_parallel: 1
  expert_parallel: 1
  use_sequence_parallel: False

model_config:
  vocab_size: 50257
  hidden_size: 2560
  num_layers: 2
  num_heads: 32
  num_experts: null
  use_gqa: False
  ffn_hidden_size: 10240
  apply_residual_connection_post_norm: False
  use_flash_attention: False
  qkv_has_bias: True
  out_proj_has_bias: True
  mask_func_type: "attn_mask_fill"
  normalization: "LayerNorm"
  layernorm_epsilon: 1.e-5
  hidden_act: "fast_gelu"
  mlp_has_bias: True

dataset_config:
  shuffle: False