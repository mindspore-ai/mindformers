optimizer_config:
  optimizer_type: AdamW
  beta1: 0.9
  beta2: 0.95
  eps: 1.e-8
  learning_rate: 1.25e-6
  weight_decay: 1.e-1
  learning_rate_scheduler_kwargs:
    warmup_steps: 200
    decay_steps: 2000
    use_cosine: True
    end_learning_rate: 1.25e-7

parallel_config:
  tensor_parallel: 1
  pipeline_stage: 1
  context_parallel: 1
  expert_parallel: 1
  use_sequence_parallel: False
  async_tensor_model_parallel_allreduce: False