training_config:
  seed: 42
  output_dir: './ckpt'
  training_iters: 4
  log_interval: 1
  eval_interval: 2
  save_interval: 2
  loss_scale: 4096
  loss_reduction: mean
  best_metric_comparison: less_equal
  eval_metric: perplexity

parallel_config:
  tensor_parallel: 2
  pipeline_stage: 1
  context_parallel: 1
  expert_parallel: 1
  use_sequence_parallel: False
  async_tensor_model_parallel_allreduce: True

model_config:
    vocab_size: 1200
    hidden_size: 128
    ffn_hidden_size: 512
    num_layers: 2
    num_heads: 8
    num_experts: null
    seq_length: 128
    position_embedding_type: rope
    # attention
    attention_type: self_attn
    use_gqa: False
    apply_residual_connection_post_norm: False
    use_flash_attention: True
    qkv_has_bias: False
    out_proj_has_bias: False
    mask_func_type: attn_mask_fill
    # layer norm
    normalization: FusedRMSNorm
    layernorm_epsilon: 1.e-5
    # hidden_act
    hidden_act: silu
    # dropout
    hidden_dropout_rate: 0.1
    attention_dropout_rate: 0.1
    # mlp
    mlp_has_bias: False
    mlp_has_gate: True
    # shared weights
    head_skip_weight_param_allocation: True
    post_norm: True
    fa_config: 
        input_layout: BNSD
    # dtype
    param_init_dtype: float32
    compute_dtype: bfloat16
    softmax_compute_dtype: float32

dataset_config:
  dataset_dir: 'data'
  batch_size: 1
  micro_batch_num: 2
  shuffle: False
  drop_remainder: True
  pad_token: 0

optimizer_config:
  optimizer_type: AdamW
  beta1: 0.9
  beta2: 0.95
  eps: 1.e-8
  learning_rate: 1.25e-6
  weight_decay: 1.e-1
  learning_rate_scheduler_kwargs:
    warmup_steps: 200
    decay_steps: 2000
    use_cosine: True
    end_learning_rate: 1.25e-7