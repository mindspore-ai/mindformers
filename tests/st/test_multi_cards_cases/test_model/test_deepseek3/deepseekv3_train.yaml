seed: 0
output_dir: './output' # path to save checkpoint/strategy
load_checkpoint: ''
src_strategy_path_or_dir: ''
auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
only_save_strategy: False
resume_training: False
use_parallel: True
run_mode: 'train'
train_precision_sync: True
load_ckpt_format: 'safetensors'
use_legacy: False
# trainer config
trainer:
  type: CausalLanguageModelingTrainer
  model_name: 'deepseekV3'

# runner config
runner_config:
  epochs: 1
  batch_size: 1
  sink_mode: True
  sink_size: 1

# optimizer
optimizer:
  type: AdamW
  betas: [0.9, 0.95]
  eps: 1.e-8

# lr schedule
lr_schedule:
  type: ConstantWarmUpLR
  learning_rate: 2.2e-4
  lr_end: 2.2e-4
  warmup_steps: 100
  total_steps: -1

# mindspore context init config
context:
  mode: 0 #0--Graph Mode; 1--Pynative Mode
  device_target: "Ascend"
  max_call_depth: 10000
  max_device_memory: "55GB"
  save_graphs: False
  save_graphs_path: "./graph"
  jit_config:
    jit_level: "O0"

parallel_config:
  data_parallel: 2
  model_parallel: 2
  pipeline_stage: 2
  expert_parallel: 2
  micro_batch_num: 2
  vocab_emb_dp: True
  use_seq_parallel: True
  gradient_aggregation_group: 4
# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
micro_batch_interleave_num: 1

# parallel context config
parallel:
  parallel_mode: 1
  gradients_mean: False
  enable_alltoall: True
  search_mode: "sharding_propagation"
  enable_parallel_optimizer: True
  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
  parallel_optimizer_config:
    gradient_accumulation_shard: False
    parallel_optimizer_threshold: 64
    optimizer_weight_shard_size: 4                    # 修改为8

# recompute config
recompute_config:
  recompute: False
  select_recompute: False
  parallel_optimizer_comm_recompute: False
  mp_comm_recompute: False
  recompute_slice_activation: False

pretrained_model_dir: ''
# model config
model:
  model_config:
    vocab_size: 32000
    seq_length: 4096
    hidden_size: 256
    num_hidden_layers: 3
    first_k_dense_replace: 1
    num_attention_heads: 8
    num_key_value_heads: 8
    max_position_embeddings: 4096
    intermediate_size: 512
    moe_intermediate_size: 512
    compute_dtype: "bfloat16"
    layernorm_compute_dtype: "float32"
    softmax_compute_dtype: "float32"
    rotary_dtype: "float32"
    params_dtype: "float32"
    router_dense_type: "float32"
    hidden_dropout: 0.1
    offset: 0
    mtp_loss_scaling_factor: 0.3
    input_sliced_sig: True
    n_routed_experts:  16
    add_bias_linear: False
    gated_linear_unit: True
    qk_layernorm: True
    aux_loss_factors: [0.0001]
    aux_loss_types: ["expert"]
    moe_token_drop_policy: probs
    moe_router_enable_expert_bias: True
    moe_router_bias_update_rate: 0.001
    moe_grouped_gemm: True

# callbacks
callbacks:
  - type: MFLossMonitor
    per_print_times: 1
  # balance topk bias with callback
  - type: TopkBiasBalanceCallback

# wrapper cell config
runner_wrapper:
  type: MFTrainOneStepCell
  scale_sense: 1.0
  use_clip_grad: True
